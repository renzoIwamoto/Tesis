{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Base DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from gymnasium.wrappers import RecordVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'BreakoutDeterministic-v4'\n",
    "GAME_NAME = ENV_NAME.split('-')[0]\n",
    "FRAME_STACK = 4                          # Número de frames apilados para representar el estado.\n",
    "GAMMA = 0.99                             # Factor de descuento para las recompensas futuras\n",
    "LEARNING_RATE = 0.00025                  # Tasa de aprendizaje para el optimizador.\n",
    "MEMORY_SIZE = 1000000                    # Tamaño de la memoria de experiencia.\n",
    "BATCH_SIZE = 32\n",
    "TRAINING_START = 50000                   # Número de pasos antes de comenzar el entrenamiento.\n",
    "INITIAL_EPSILON = 1.0\n",
    "FINAL_EPSILON = 0.1\n",
    "EXPLORATION_STEPS = 250000 #1000000      # Número de pasos para disminuir epsilon.\n",
    "UPDATE_TARGET_FREQUENCY = 10000          # Frecuencia para actualizar el modelo objetivo.\n",
    "SAVE_FREQUENCY = 100000                  # Frecuencia para guardar el modelo.\n",
    "EVALUATION_FREQUENCY = 50000             # Frecuencia para evaluar el agente.\n",
    "NUM_EVALUATION_EPISODES = 10             # Número de episodios para la evaluación.\n",
    "EPISODES = 10000                         # Número total de episodios para el entrenamiento.\n",
    "TRAIN_FREQUENCY = 4                      # Entrenar cada 4 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for gpu in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME_FOLDER = f'{GAME_NAME}_results'\n",
    "os.makedirs(GAME_FOLDER, exist_ok=True)\n",
    "\n",
    "# Crear subcarpetas\n",
    "MODELS_FOLDER = os.path.join(GAME_FOLDER, 'models')\n",
    "REPLAYS_FOLDER = os.path.join(GAME_FOLDER, 'replays')\n",
    "VIDEOS_FOLDER = os.path.join(GAME_FOLDER, 'videos')\n",
    "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "os.makedirs(REPLAYS_FOLDER, exist_ok=True)\n",
    "os.makedirs(VIDEOS_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def preprocess_frame(frame):\n",
    "    gray = tf.image.rgb_to_grayscale(frame)\n",
    "    resized = tf.image.resize(gray, [84, 84])\n",
    "    normalized = resized / 255.0\n",
    "    return normalized[:,:,0]  # Asegúrate de que sea 2D\n",
    "\n",
    "def stack_frames(stacked_frames, frame, is_new_episode):\n",
    "    frame = preprocess_frame(frame)\n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([frame] * FRAME_STACK, maxlen=FRAME_STACK)\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "    stacked = np.stack(stacked_frames, axis=-1)  # Asegúrate de que sea 3D (84, 84, 4)\n",
    "    return stacked, stacked_frames  # Devuelve también stacked_frames para actualizar el deque\n",
    "\n",
    "def evaluate_agent(env, agent, num_episodes):\n",
    "    total_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        stacked_frames = deque(maxlen=FRAME_STACK)\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = agent.act(np.expand_dims(state, axis=0))\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "        total_rewards.append(episode_reward)\n",
    "    return np.mean(total_rewards)\n",
    "\n",
    "def plot_training_progress(scores, avg_q_values, losses, game_name):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "    ax1.plot(scores)\n",
    "    ax1.set_title(f'{game_name} - Episode Scores')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Score')\n",
    "\n",
    "    ax2.plot(avg_q_values)\n",
    "    ax2.set_title(f'{game_name} - Average Q-values')\n",
    "    ax2.set_xlabel('Step')\n",
    "    ax2.set_ylabel('Avg Q-value')\n",
    "\n",
    "    ax3.plot(losses)\n",
    "    ax3.set_title(f'{game_name} - Loss')\n",
    "    ax3.set_xlabel('Training Step')\n",
    "    ax3.set_ylabel('Loss')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(GAME_FOLDER, f'training_progress_{game_name}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "    state_shape = (84, 84, FRAME_STACK)\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_shape, action_size)\n",
    "    stacked_frames = deque(maxlen=FRAME_STACK)\n",
    "    \n",
    "    scores = []\n",
    "    total_steps = 0\n",
    "\n",
    "    for episode in range(EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "\n",
    "        for time_step in range(20000):\n",
    "            action = agent.act(np.expand_dims(state, axis=0))\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            if done:\n",
    "                reward = -10  # Reward negativo cuando el episodio termina\n",
    "            \n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            total_steps += 1\n",
    "            episode_steps += 1\n",
    "\n",
    "            #if episode_steps % 100 == 0:\n",
    "            #    print(f\"Episode: {episode}, Step: {episode_steps}, Total Steps: {total_steps}\")\n",
    "\n",
    "            if len(agent.memory) >= BATCH_SIZE and total_steps % TRAIN_FREQUENCY == 0:\n",
    "                agent.replay()\n",
    "                agent.update_epsilon(total_steps)\n",
    "\n",
    "            if total_steps % UPDATE_TARGET_FREQUENCY == 0:\n",
    "                agent.update_target_model()\n",
    "\n",
    "            if total_steps % SAVE_FREQUENCY == 0:\n",
    "                agent.save(os.path.join(MODELS_FOLDER, f'dqn_model_{GAME_NAME}_step_{total_steps}'))\n",
    "                with open(os.path.join(REPLAYS_FOLDER, f'experience_replay_{GAME_NAME}_step_{total_steps}.pkl'), 'wb') as f:\n",
    "                    pickle.dump(agent.memory, f)\n",
    "\n",
    "            if total_steps % EVALUATION_FREQUENCY == 0:\n",
    "                eval_score = evaluate_agent(env, agent, NUM_EVALUATION_EPISODES)\n",
    "                print(f\"Step: {total_steps}, Evaluation Score: {eval_score}\")\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores.append(episode_reward)\n",
    "        print(f\"Episode: {episode}, Score: {episode_reward}, Epsilon: {agent.epsilon:.2f}, Steps: {episode_steps}\")\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            plot_training_progress(scores, agent.q_values_history, agent.loss_history, GAME_NAME)\n",
    "\n",
    "    # Guardar el modelo final y el experience replay\n",
    "    agent.save(os.path.join(MODELS_FOLDER, f'dqn_model_{GAME_NAME}_final'))\n",
    "    with open(os.path.join(REPLAYS_FOLDER, f'experience_replay_{GAME_NAME}_final.pkl'), 'wb') as f:\n",
    "        pickle.dump(agent.memory, f)\n",
    "\n",
    "    # Grabar video del agente entrenado\n",
    "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "    env = RecordVideo(env, VIDEOS_FOLDER)\n",
    "    state, _ = env.reset()\n",
    "    stacked_frames = deque(maxlen=FRAME_STACK)\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(np.expand_dims(state, axis=0))\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        state = next_state\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
